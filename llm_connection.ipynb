{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am retrieving the embeddings from qdrant, and then creating a prompt.\n",
    "Then the prompt is sent to an LLM, which returns the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(19139) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting together\n",
      "  Downloading together-1.3.5-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from together) (3.11.10)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from together) (8.1.7)\n",
      "Collecting eval-type-backport<0.3.0,>=0.1.3 (from together)\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from together) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from together) (2.0.2)\n",
      "Collecting pillow<11.0.0,>=10.3.0 (from together)\n",
      "  Downloading pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from together) (18.1.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from together) (2.10.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from together) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from together) (13.9.4)\n",
      "Collecting tabulate<0.10.0,>=0.9.0 (from together)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from together) (4.67.1)\n",
      "Collecting typer<0.14,>=0.9 (from together)\n",
      "  Downloading typer-0.13.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.18.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (2.18.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from typer<0.14,>=0.9->together) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/anshharjai/anaconda3/envs/ai_project/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n",
      "Downloading together-1.3.5-py3-none-any.whl (70 kB)\n",
      "Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.13.1-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: tabulate, pillow, eval-type-backport, typer, together\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.0.0\n",
      "    Uninstalling pillow-11.0.0:\n",
      "      Successfully uninstalled pillow-11.0.0\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.15.1\n",
      "    Uninstalling typer-0.15.1:\n",
      "      Successfully uninstalled typer-0.15.1\n",
      "Successfully installed eval-type-backport-0.2.0 pillow-10.4.0 tabulate-0.9.0 together-1.3.5 typer-0.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert query to embedding\n",
    "# retrieve 5 most similar chunks for this query\n",
    "# make a prompt with query and chunks\n",
    "# send prompt to llm and get response\n",
    "# print response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import spacy\n",
    "from pymongo import MongoClient\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_embedding(text):\n",
    "    \"\"\"Convert text to embeddings using SpaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is ros2?\"\n",
    "query_embedding = text_to_embedding(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since qdrant is not working in python notebook, I have stored its value in the following list for the above query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [{'score': 0.83282745, 'chunk': \"we can leverage the experience of community members, and keep the varied use cases of ros in mind. etiquette assume 'good faith': it's easy to mis interpret the meaning or tone of comments on the internet. assuming good faith gives the benefit of the doubt to those trying to help you, avoiding: insulting well meaning community members, and poisoning the mood. assuming 'good faith' when responding almost always works better even if the original response was not in fact in good faith. please don't send your question more than once: the question was seen. if you didn't get a response then likely nobody has had time to answer you. alternatively, it could be that nobody knows the answer. in any case, sending it again is poor form and akin to shouting and is likely to aggravate a large number of people. this also applies to crossposting. try to pick the forum which you think matches best and ask there. if you are referred to a new forum, provide a link to the old discussion. on robotics stack exchange __ you can edit your question to provide more details. the more details that you include in your question the easier it\"}, {'score': 0.82495856, 'chunk': 'carrot is now, we will make the second turtle go to where the first carrot was 5 seconds ago. edit the lookuptransform() call in turtle_tf2_listener.cpp file to .. code block:: c++ rclcpp::time when = this >get_clock() >now() rclcpp::duration(5, 0); try { t = tf_buffer_ >lookuptransform( toframerel, fromframerel, when, 50ms); } catch (const tf2::transformexception & ex) { now if you run this, during the first 5 seconds, the second turtle would not know where to go because we do not yet have a 5 second history of poses of the carrot. but what happens after these 5 seconds? build the package then let\\'s just give it a try: .. code block:: console ros2 launch learning_tf2_cpp turtle_tf2_fixed_frame_demo_launch.py .. image:: images/turtlesim_delay1.png you should now notice that your turtle is driving around uncontrollably like in this screenshot. let\\'s try to understand reason behind that behavior. . in our code we asked tf2 the following question: \"what was the pose of carrot1 5 seconds ago, relative to turtle2 5 seconds ago?\". this means we are controlling the second turtle based on where it was 5 seconds ago as well as where the first carrot was 5 seconds ago. . however, what we really want to'}, {'score': 0.8222663, 'chunk': 'timer to fire at about once per second. also you may have noticed that the first message (with value 0 ) does not have a corresponding \"received message ...\" line. this is because publish/subscribe is \"best effort\" and we do not have any \"latching\" like behavior enabled. this means that if the publisher publishes a message before the subscription has been established, the subscription will not receive that message. this race condition can result in the first few messages being lost. in this case, since they only come once per second, usually only the first message is lost. finally, you can see that \"published message...\" and \"received message ...\" lines with the same value also have the same address. this shows that the address of the message being received is the same as the one that was published and that it is not a copy. this is because we\\'re publishing and subscribing with std::unique_ptr s which allow ownership of a message to be moved around the system safely. you can also publish and subscribe with const & and std::shared_ptr , but zero copy will not occur in that case. the cyclic pipeline demo ^^^^^^^^^^^^^^^^^^^^^^^^ this demo is similar to the'}, {'score': 0.81889766, 'chunk': \"be specific enough to identify what the package does. for example, a motion planner is not called planner. if it implements the wavefront propagation algorithm, it might be called wavefront_planner. there's obviously tension between making a name specific and keeping it from becoming overly verbose. using catchall names such as utils should be avoided as they do not scope what goes into the package or what should be outside the package. to check whether a name is taken, consult __. if you'd like your repository included in that list, see the rosdistro contributing guide __. our goal is to develop a canonical set of tools for making robots do interesting things. the package name should tell you what the package does, not where it came from. it should be possible for us, as a community, to make this work. an ubuntu distribution offers approximately 33,000 packages without inserting origin or authorship into names. prefixing a package name is recommended only when the package is not meant to be used more widely (e.g., packages that are specific to the pr2 robot use the pr2_ prefix). you might prefix the package name when forking an existing package, but again, the prefix would\"}, {'score': 0.81763595, 'chunk': \"the leg to be parallel to the z axis, we rotate the visual part pi/2 around the y axis. .. code block:: console ros2 launch urdf_tutorial display.launch.py model:=urdf/03 origins.urdf .. image:: https://raw.githubusercontent.com/ros/urdf_tutorial/ros2/images/origins.png :width: 800 :alt: origins screenshot the launch file runs packages that will create tf frames for each link in your model based on your urdf. rviz uses this information to figure out where to display each shape. if a tf frame does not exist for a given urdf link, then it will be placed at the origin in white (ref. related question _). material girl “alright,” i hear you say. “that’s very cute, but not everyone owns a b21. my robot and r2d2 are not red!” that’s a good point. let’s take a look at the material tag. [source: 04 materials.urdf] _ .. code block:: xml the body is now blue. we’ve defined a new material called “blue”, with the red, green, blue and alpha channels defined as 0,0,0.8 and 1 respectively. all of the values can be in the range [0,1]. this material is then referenced by the base_link's visual element. the white material is defined similarly. you could also define the material tag from within the\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\"\n",
    "for c in context:\n",
    "    context_text += c['chunk'] + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can leverage the experience of community members, and keep the varied use cases of ros in mind. etiquette assume 'good faith': it's easy to mis interpret the meaning or tone of comments on the internet. assuming good faith gives the benefit of the doubt to those trying to help you, avoiding: insulting well meaning community members, and poisoning the mood. assuming 'good faith' when responding almost always works better even if the original response was not in fact in good faith. please don't send your question more than once: the question was seen. if you didn't get a response then likely nobody has had time to answer you. alternatively, it could be that nobody knows the answer. in any case, sending it again is poor form and akin to shouting and is likely to aggravate a large number of people. this also applies to crossposting. try to pick the forum which you think matches best and ask there. if you are referred to a new forum, provide a link to the old discussion. on robotics stack exchange __ you can edit your question to provide more details. the more details that you include in your question the easier it carrot is now, we will make the second turtle go to where the first carrot was 5 seconds ago. edit the lookuptransform() call in turtle_tf2_listener.cpp file to .. code block:: c++ rclcpp::time when = this >get_clock() >now() rclcpp::duration(5, 0); try { t = tf_buffer_ >lookuptransform( toframerel, fromframerel, when, 50ms); } catch (const tf2::transformexception & ex) { now if you run this, during the first 5 seconds, the second turtle would not know where to go because we do not yet have a 5 second history of poses of the carrot. but what happens after these 5 seconds? build the package then let's just give it a try: .. code block:: console ros2 launch learning_tf2_cpp turtle_tf2_fixed_frame_demo_launch.py .. image:: images/turtlesim_delay1.png you should now notice that your turtle is driving around uncontrollably like in this screenshot. let's try to understand reason behind that behavior. . in our code we asked tf2 the following question: \"what was the pose of carrot1 5 seconds ago, relative to turtle2 5 seconds ago?\". this means we are controlling the second turtle based on where it was 5 seconds ago as well as where the first carrot was 5 seconds ago. . however, what we really want to timer to fire at about once per second. also you may have noticed that the first message (with value 0 ) does not have a corresponding \"received message ...\" line. this is because publish/subscribe is \"best effort\" and we do not have any \"latching\" like behavior enabled. this means that if the publisher publishes a message before the subscription has been established, the subscription will not receive that message. this race condition can result in the first few messages being lost. in this case, since they only come once per second, usually only the first message is lost. finally, you can see that \"published message...\" and \"received message ...\" lines with the same value also have the same address. this shows that the address of the message being received is the same as the one that was published and that it is not a copy. this is because we're publishing and subscribing with std::unique_ptr s which allow ownership of a message to be moved around the system safely. you can also publish and subscribe with const & and std::shared_ptr , but zero copy will not occur in that case. the cyclic pipeline demo ^^^^^^^^^^^^^^^^^^^^^^^^ this demo is similar to the be specific enough to identify what the package does. for example, a motion planner is not called planner. if it implements the wavefront propagation algorithm, it might be called wavefront_planner. there's obviously tension between making a name specific and keeping it from becoming overly verbose. using catchall names such as utils should be avoided as they do not scope what goes into the package or what should be outside the package. to check whether a name is taken, consult __. if you'd like your repository included in that list, see the rosdistro contributing guide __. our goal is to develop a canonical set of tools for making robots do interesting things. the package name should tell you what the package does, not where it came from. it should be possible for us, as a community, to make this work. an ubuntu distribution offers approximately 33,000 packages without inserting origin or authorship into names. prefixing a package name is recommended only when the package is not meant to be used more widely (e.g., packages that are specific to the pr2 robot use the pr2_ prefix). you might prefix the package name when forking an existing package, but again, the prefix would the leg to be parallel to the z axis, we rotate the visual part pi/2 around the y axis. .. code block:: console ros2 launch urdf_tutorial display.launch.py model:=urdf/03 origins.urdf .. image:: https://raw.githubusercontent.com/ros/urdf_tutorial/ros2/images/origins.png :width: 800 :alt: origins screenshot the launch file runs packages that will create tf frames for each link in your model based on your urdf. rviz uses this information to figure out where to display each shape. if a tf frame does not exist for a given urdf link, then it will be placed at the origin in white (ref. related question _). material girl “alright,” i hear you say. “that’s very cute, but not everyone owns a b21. my robot and r2d2 are not red!” that’s a good point. let’s take a look at the material tag. [source: 04 materials.urdf] _ .. code block:: xml the body is now blue. we’ve defined a new material called “blue”, with the red, green, blue and alpha channels defined as 0,0,0.8 and 1 respectively. all of the values can be in the range [0,1]. this material is then referenced by the base_link's visual element. the white material is defined similarly. you could also define the material tag from within the \n"
     ]
    }
   ],
   "source": [
    "print(context_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROS2 (Robot Operating System 2) is an open-source software framework that enables the development of robotic applications. It's the successor to ROS (Robot Operating System), which was first released in 2003.\n",
      "\n",
      "ROS2 is designed to provide a flexible and scalable architecture for building robotic systems, allowing developers to focus on the development of their robot's functionality rather than the underlying infrastructure. It provides a set of tools, libraries, and conventions that aim to simplify the task of creating robot applications.\n",
      "\n",
      "Some of the key features of ROS2 include:\n",
      "\n",
      "1. **Distributed architecture**: ROS2 allows multiple nodes (programs) to communicate with each other, enabling the creation of distributed robotic systems.\n",
      "2. **Real-time capabilities**: ROS2 provides support for real-time systems, enabling robots to respond quickly to changing environments.\n",
      "3. **Security**: ROS2 includes built-in security features, such as encryption and access control, to ensure the integrity and confidentiality of robotic systems.\n",
      "4. **Multi-platform support**: ROS2 can run on a variety of platforms, including Linux, Windows, and macOS.\n",
      "5. **Modular design**: ROS2's modular architecture allows developers to easily add or remove components as needed, making it easier to maintain and update robotic systems.\n",
      "\n",
      "ROS2 is widely used in various fields, including robotics, autonomous vehicles, drones, and industrial automation. Its flexibility, scalability, and open-source nature make it an attractive choice for developers and researchers working on robotic projects.\n",
      "\n",
      "Would you like to know more about ROS2 or is there something specific you'd like to explore?\n"
     ]
    }
   ],
   "source": [
    "together_ai_api = \"21003311b33c7dddadb89d6c0c7b42e81d8e39c866f672692b15a404e32351e6\"\n",
    "\n",
    "import os\n",
    "from together import Together\n",
    "\n",
    "client = Together(api_key=together_ai_api)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant for a robotics company. Your task is to answer questions about ROS2, Nav2, MoveIt2, and Gazebo. Use the provided data to generate responses for the given question. Your responses should be informative and accurate.\"},\n",
    "              {\"role\": \"user\", \"content\": query, \"context\": context_text},],\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
